# -*- coding: utf-8 -*-
"""Earth_Mark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCvGGP9z2KAQVC_AwmIgRYX3vGADDwNB
"""

import re
import random
import numpy as np
import pandas as pd
from io import StringIO
from sklearn.svm import SVR
import matplotlib.pyplot as plt
from scipy.stats import uniform
from sklearn.cluster import KMeans
import matplotlib.colors as mcolors
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV

data = pd.read_csv("data.csv", encoding='latin-1')

def extract_country(location):
    if re.search(r'[^\w\s,]', location):
        match = re.search(r'(?<=,\s)(\w+)$', location)
        if match:
            country = match.group(0)
        else:
            country = "Unknown"
    else:
        country = location
    return country

data['Country'] = data['Location'].apply(extract_country)

data.to_csv("data_with_country.csv", index=False)

data = pd.read_csv("data_with_country.csv")

data['Country'] = data['Location'].apply(lambda x: x.split(",")[-1].strip())

data.to_csv("preprocessed_data.csv", index=False)

earthquake_counts = data['Country'].value_counts()

earthquake_counts_filtered = earthquake_counts[earthquake_counts > 5]

colormap = plt.cm.Reds

norm = mcolors.Normalize(vmin=0, vmax=earthquake_counts_filtered.max())

fig, ax = plt.subplots(figsize=(12, 6))

bars = ax.bar(earthquake_counts_filtered.index, earthquake_counts_filtered, color=colormap(norm(earthquake_counts_filtered)))

for bar in bars:
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, str(int(bar.get_height())), ha='center', va='bottom')

ax.set_xlabel('Country', fontweight='bold')
ax.set_ylabel('Number of Occurrences', fontweight='bold')

sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax)
cbar.set_label('Number of Occurrences')

plt.xticks(rotation=20, ha='right')

plt.tight_layout()

plt.show()

earthquake_counts = data['Country'].value_counts()
countries_with_more_than_5_earthquakes = earthquake_counts[earthquake_counts > 5].index

filtered_data = data[data['Country'].isin(countries_with_more_than_5_earthquakes)]

tzinfos = {'BST': 0}

grouped_data = filtered_data.groupby('Country')

plt.figure(figsize=(12, 6))
for country, group in grouped_data:
    group['Time'] = pd.to_datetime(group['Time'], errors='coerce', utc=True, format='%Y-%m-%d %H:%M:%S', exact=False)
    group.set_index('Time', inplace=True)
    monthly_counts = group.resample('M').size()
    monthly_counts.plot(label=country)

plt.xlabel('Time', fontweight='bold')
plt.ylabel('Number of Occurrences', fontweight='bold')
plt.legend(title='Country', bbox_to_anchor=(0.74, 1), loc='upper left')
plt.tight_layout()
plt.show()

earthquake_counts = data['Country'].value_counts()
countries_with_more_than_5_earthquakes = earthquake_counts[earthquake_counts > 5].index

filtered_data = data[data['Country'].isin(countries_with_more_than_5_earthquakes)]

grouped_data = filtered_data.groupby('Country')

country_statistics = pd.DataFrame()
country_statistics['Total Earthquakes'] = grouped_data.size()

magnitude_numeric = filtered_data['Magnitude'].str.extract(r'(\d+\.\d+|\d+)').astype(float)

country_statistics['Average Magnitude'] = magnitude_numeric.groupby(filtered_data['Country']).mean()
country_statistics['Minimum Magnitude'] = magnitude_numeric.groupby(filtered_data['Country']).min()
country_statistics['Maximum Magnitude'] = magnitude_numeric.groupby(filtered_data['Country']).max()
country_ranking = country_statistics.sort_values(by='Total Earthquakes', ascending=False)

print("Ranking of Earthquake-Prone Areas/Regions:")
print(country_ranking)

# Load the CSV file
file_path = '/content/preprocessed_data.csv'
data = pd.read_csv(file_path)

# Display the first few rows to understand the structure of the data
print(data.head())

# Extract numeric part of Magnitude
data['Magnitude'] = pd.to_numeric(data['Magnitude'].str.extract('(\d+\.\d+)')[0])

# Calculate the mean, median, and standard deviation of Magnitude for each country
statistics = data.groupby('Country')['Magnitude'].agg(['mean', 'median', 'std']).reset_index()

# Display the statistics
print(statistics)

countries = ['Bangladesh', 'China', 'Nepal', 'Bhutan', 'India']

tzinfos = {'BST': 0}

for country in countries:
    print(f"Predictions for {country}:")

    country_data = data[data['Country'] == country].copy()

    country_data['Time'] = pd.to_datetime(country_data['Time'], errors='coerce', utc=True, format='%Y-%m-%d %H:%M:%S', exact=False)
    country_data['Month'] = country_data['Time'].dt.month

    earthquake_counts_by_month = country_data.groupby('Month').size()

    country_data['Magnitude'] = country_data['Magnitude'].apply(lambda x: float(re.findall(r'[\d.]+', x)[0]) if re.findall(r'[\d.]+', x) else None)
    earthquake_stats_by_month = country_data.groupby('Month')['Magnitude'].agg(['max', 'min', 'mean'])

    param_dist = {
        'kernel': ['linear', 'poly', 'rbf'],
        'C': uniform(loc=0.1, scale=10),
        'gamma': ['scale', 'auto'],
        'epsilon': uniform(loc=0.01, scale=0.1)
    }

    X_counts = earthquake_counts_by_month.index.values.reshape(-1, 1)
    y_counts = earthquake_counts_by_month.values
    svr_counts_model = RandomizedSearchCV(SVR(), param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
    svr_counts_model.fit(X_counts, y_counts)

    X_max_magnitude = earthquake_stats_by_month.index.values.reshape(-1, 1)
    y_max_magnitude = earthquake_stats_by_month['max'].values
    svr_max_magnitude_model = RandomizedSearchCV(SVR(), param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
    svr_max_magnitude_model.fit(X_max_magnitude, y_max_magnitude)

    X_min_magnitude = earthquake_stats_by_month.index.values.reshape(-1, 1)
    y_min_magnitude = earthquake_stats_by_month['min'].values
    svr_min_magnitude_model = RandomizedSearchCV(SVR(), param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
    svr_min_magnitude_model.fit(X_min_magnitude, y_min_magnitude)

    X_avg_magnitude = earthquake_stats_by_month.index.values.reshape(-1, 1)
    y_avg_magnitude = earthquake_stats_by_month['mean'].values
    svr_avg_magnitude_model = RandomizedSearchCV(SVR(), param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
    svr_avg_magnitude_model.fit(X_avg_magnitude, y_avg_magnitude)

    months = list(range(1, 13))

    predicted_earthquake_counts = svr_counts_model.predict([[month] for month in months])
    predicted_max_magnitudes = svr_max_magnitude_model.predict([[month] for month in months])
    predicted_min_magnitudes = svr_min_magnitude_model.predict([[month] for month in months])
    predicted_avg_magnitudes = svr_avg_magnitude_model.predict([[month] for month in months])

    min_length = min(len(months), len(earthquake_counts_by_month), len(earthquake_stats_by_month))
    predictions_table = pd.DataFrame({
        'Months': months[:min_length],
        f'Predicted EQ Counts ({country[:3]})': predicted_earthquake_counts[:min_length],
        f'Historical EQ Counts ({country[:3]})': earthquake_counts_by_month.values[:min_length],
        f'Predicted Max Mag ({country[:3]})': predicted_max_magnitudes[:min_length],
        f'Historical Max Mag ({country[:3]})': country_data.groupby('Month')['Magnitude'].max().values[:min_length],
        f'Predicted Min Mag ({country[:3]})': predicted_min_magnitudes[:min_length],
        f'Historical Min Mag ({country[:3]})': country_data.groupby('Month')['Magnitude'].min().values[:min_length],
        f'Predicted Avg Mag ({country[:3]})': predicted_avg_magnitudes[:min_length],
        f'Historical Avg Mag ({country[:3]})': country_data.groupby('Month')['Magnitude'].mean().values[:min_length]
    })

    print(predictions_table)
    print('\n')

countries = ['Bangladesh', 'China', 'Nepal', 'Bhutan', 'India']

zinfos = {'BST': 0}

for country in countries:
    print(f"Analysis for {country}:")

    data_country = data[data['Country'] == country].copy()

    data_country['Time'] = pd.to_datetime(data_country['Time'], errors='coerce', utc=True)
    data_country['Month'] = data_country['Time'].dt.month

    def extract_magnitude(x):
        magnitudes = re.findall(r'[\d.]+', x)
        if magnitudes:
            return float(magnitudes[0])
        else:
            return None

    data_country['Magnitude'] = data_country['Magnitude'].apply(extract_magnitude)

    data_country = data_country.dropna(subset=['Magnitude'])

    kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')
    data_country['Cluster'] = kmeans.fit_predict(data_country[['Magnitude']])

    plt.figure(figsize=(10, 6))
    plt.scatter(data_country['Month'], data_country['Magnitude'], c=data_country['Cluster'], cmap='viridis')
    #plt.title(f'Clusters of Earthquakes in {country}')
    plt.xlabel('Month')
    plt.ylabel('Magnitude')
    plt.colorbar(label='Cluster')
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(10, 6))
    data_country['Month'].value_counts().sort_index().plot(kind='bar')
    #plt.title(f'Distribution of Earthquakes Over Months in {country}')
    plt.xlabel('Month')
    plt.ylabel('Number of Earthquakes')
    plt.xticks(rotation=45)
    plt.grid(axis='y')
    plt.show()

    X = data_country[['Month']]
    y = data_country['Magnitude']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    svr_model = SVR(kernel='rbf')
    svr_model.fit(X_train, y_train)

    svr_predictions = svr_model.predict(X_test)
    svr_mse = mean_squared_error(y_test, svr_predictions)
    print(f"SVR Mean Squared Error for {country}: {svr_mse}")

    severity_estimation = data_country.groupby('Cluster')['Magnitude'].max()

    print(f"Severity Estimation for {country}:")
    print(severity_estimation)

    sample_months = random.sample(range(1, 13), 5)
    sample_predictions = svr_model.predict([[month] for month in sample_months])

    print(f"\nSample Test Cases for {country}:")
    for month, prediction in zip(sample_months, sample_predictions):
        print(f"Month: {month}, Predicted Magnitude: {prediction:.2f}")

    historical_max_magnitudes = data_country.groupby('Month')['Magnitude'].max()
    for month, prediction in zip(sample_months, sample_predictions):
        historical_max_magnitude = historical_max_magnitudes.get(month, None)
        if historical_max_magnitude is not None:
            print(f"Month: {month}, Predicted Magnitude: {prediction:.2f}, Historical Max Magnitude: {historical_max_magnitude:.2f}")
        else:
            print(f"No historical data available for month {month}")

    print('\n')